{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd2f44f",
   "metadata": {},
   "source": [
    "### Word Frequency in Classic Novels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863a7e0",
   "metadata": {},
   "source": [
    "##### 1. Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed791fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4f401",
   "metadata": {},
   "source": [
    "##### 2. List constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6f12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the text file to analyze\n",
    "URL = \"https://www.gutenberg.org/cache/epub/15/pg15-images.html\"\n",
    "\n",
    "# Number of most common words to display\n",
    "TOP_N_WORDS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064febb",
   "metadata": {},
   "source": [
    "##### 3. Set up NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fcf428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "\n",
    "except nltk.downloader.DownloaderError:\n",
    "    print(\"NLTK stopwords not found. Downloading now...\")\n",
    "    try:\n",
    "        nltk.download('stopwords')\n",
    "        print(\"NLTK stopwords downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK stopwords: {e}\")\n",
    "        print(\"Please ensure you have an internet connection and try again.\")\n",
    "        sys.exit(1) # Exit if stopwords cannot be downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370e425",
   "metadata": {},
   "source": [
    "##### 4. Fetch text from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d880d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_text_from_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Fetches HTML content from a given URL and extracts text from <p> tags.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the web page to fetch.\n",
    "        \n",
    "    Returns:\n",
    "\n",
    "        str | None: The extracted text as a single string, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find all <p> tags and extract their text, joining them into one string\n",
    "        paragraphs = soup.find_all('p')\n",
    "        extracted_text = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "        return extracted_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching content from {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occured during text extraction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795cf856",
   "metadata": {},
   "source": [
    "##### 5. Clean and Tokenize Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02fb774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize_text (text: str, remove_stopwords: bool = True) -> list[str]:\n",
    "    \"\"\"\n",
    "    Cleans the input text by converting to lowercase, removing punctuations, and optionally\n",
    "    removing English stopwords.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text string.\n",
    "        remove_stopwords (bool): If True, remove common English stopwords.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of cleaned and tokenized words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert text to lowercase \n",
    "    cleaned_text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    # str.maketrans creates a translation table: '' for remoal\n",
    "\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = cleaned_text.split()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        # Get the set of English stopwords for efficient lookup\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Filter out stopwords\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9692db",
   "metadata": {},
   "source": [
    "##### 6. Count Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96cf8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_frequencies(word_list: list[str]) -> Counter:\n",
    "    \"\"\"\n",
    "        Counts the frequency of each word in a lst.\n",
    "    Args:\n",
    "        word_list (list[str]): A list of words.\n",
    "\n",
    "    Returns:\n",
    "        Counter: A Counter object mapping words to their frequencies.\n",
    "    \"\"\"\n",
    "    return Counter(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f36c98",
   "metadata": {},
   "source": [
    "##### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e27ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Word Frequency Analysis from https://www.gutenberg.org/cache/epub/15/pg15-images.html ---\n",
      "\n",
      "-- Raw Word Frequencies (before cleaning) ---\n",
      "[('the', 13662), ('of', 6494), ('and', 5899), ('a', 4477), ('to', 4448), ('in', 3824), ('that', 2679), ('his', 2426), ('I', 1715), ('with', 1647)]\n",
      "\n",
      "--- Cleaning Word Frequencies (after cleaning and stopwords removal\n",
      "[('whale', 894), ('one', 880), ('like', 570), ('upon', 560), ('old', 439), ('would', 427), ('man', 414), ('ahab', 400), ('ye', 394), ('ship', 369)]\n",
      "\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Starting Word Frequency Analysis from {URL} ---\")\n",
    "\n",
    "    # Step 1: Fetch text from the URL\n",
    "    raw_book_text =  fetch_text_from_url(URL)\n",
    "\n",
    "    if raw_book_text:\n",
    "        print(\"\\n-- Raw Word Frequencies (before cleaning) ---\")\n",
    "        # Step 2a: Tokenize raw text (without cleaning)\n",
    "        raw_word_list = raw_book_text.split()\n",
    "        # Step 3a: Count frequencies for raw words\n",
    "        raw_word_frequencies = count_word_frequencies(raw_word_list)\n",
    "        # Print the most common words from the raw text\n",
    "        print(raw_word_frequencies.most_common(TOP_N_WORDS))\n",
    "\n",
    "        print(\"\\n--- Cleaning Word Frequencies (after cleaning and stopwords removal\")\n",
    "        # Step 2b: Clean and tokenize text (with stopword removal)\n",
    "        cleaned_tokens = clean_and_tokenize_text(raw_book_text, remove_stopwords=True)\n",
    "        # Step 3b: Count frequencies for cleaned tokens\n",
    "        nlp_word_frequencies = count_word_frequencies(cleaned_tokens)\n",
    "        # Print the most common words from the cleaned text\n",
    "        print(nlp_word_frequencies.most_common(TOP_N_WORDS))\n",
    "    else:\n",
    "        print(\"Could not retrieve text. Exiting.\")\n",
    "    \n",
    "    print(\"\\n--- Analysis Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
